# -*- coding: utf-8 -*-
"""Llamamistral7b.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13PDhKeZBxIw5GTycYwe3kXrxTAiYHwKe
"""

!pip install mysql-connector-python fastapi nest-asyncio uvicorn pyngrok requests

!pip install pandas openpyxl mysql-connector-python

pip install ctransformers

import mysql.connector

conn = mysql.connector.connect(
    host="0.tcp.in.ngrok.io",
    port=16582,
    user="root",
    password="dxxxxxxx",
    database="office"
)

cursor = conn.cursor()
print(" Connected to MariaDB!")

cursor.execute("SELECT * FROM shipment_view LIMIT 10")
rows = cursor.fetchall()

for row in rows:
    print(row)

!pip install ctransformers
!apt-get install -y libopenblas-dev

from huggingface_hub import snapshot_download

model_path = snapshot_download(
    repo_id="TheBloke/Mistral-7B-Instruct-v0.1-GGUF",
    allow_patterns="mistral-7b-instruct-v0.1.Q4_K_M.gguf",
    local_dir="/content/mistral-gguf"
)

import re
from ctransformers import AutoModelForCausalLM

view_schema = """CREATE VIEW shipment_summary (
    product_id INT,
    product_code VARCHAR(10),
    product_name VARCHAR(50),
    product_type VARCHAR(50),
    product_site VARCHAR(50),
    YEAR INT,
    qtr VARCHAR(5),
    month VARCHAR(15),
    plan DECIMAL(10,2),
    actual DECIMAL(10,2)
);"""

def make_prompt(schema, question):
    return f"""### Task
You are an expert in SQL. Generate a valid MariaDB SQL query for the question using the given schema.

### View Schema
{schema}

### Question
{question}

### SQL
"""

from ctransformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "/content/mistral-gguf",
    model_file="mistral-7b-instruct-v0.1.Q4_K_M.gguf",
    model_type="mistral"
)

import re

def generate_sql(question, schema):
    prompt = make_prompt(schema, question)
    response = model(prompt)
    match = re.search(r"``````", response, re.DOTALL | re.IGNORECASE)
    if match:
        sql = match.group(1).strip()
    else:
        sql = response.strip().split("### SQL")[-1].strip()
    return sql

view_schema = """CREATE VIEW shipment_summary (
    product_id INT,
    product_code VARCHAR(10),
    product_name VARCHAR(50),
    product_type VARCHAR(50),
    product_site VARCHAR(50),
    YEAR INT,
    qtr VARCHAR(5),
    month VARCHAR(15),
    plan DECIMAL(10,2),
    actual DECIMAL(10,2)
);"""

question = "Write a SQL query to get the total actual shipments per product_type for the year 2024 from the shipment_view table."

sql_query = generate_sql(question, view_schema)
print("Generated SQL:\n", sql_query)

create_table_query = """
CREATE TABLE IF NOT EXISTS chat_history (
    session_id VARCHAR(255),
    user_message TEXT,
    bot_response TEXT,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (session_id, timestamp)
) ENGINE=InnoDB;
"""
cursor.execute(create_table_query)
conn.commit()
print("Chat history table ready")

def save_chat(session_id, user_message, bot_response):
    insert_query = """
        INSERT INTO chat_history (session_id, user_message, bot_response)
        VALUES (%s, %s, %s)
    """
    cursor.execute(insert_query, (session_id, user_message, bot_response))
    conn.commit()

def load_chat_history(session_id, max_turns=5):
    select_query = """
        SELECT user_message, bot_response FROM chat_history
        WHERE session_id=%s
        ORDER BY timestamp DESC
        LIMIT %s
    """
    cursor.execute(select_query, (session_id, max_turns))
    rows = cursor.fetchall()[::-1]
    history = ""
    for user_msg, bot_resp in rows:
        history += f"User: {user_msg}\nBot: {bot_resp}\n"
    return history

def chatbot_response(session_id, user_message, schema):

    history = load_chat_history(session_id)

    prompt = (
        f"{history}"
        f"You are an expert in SQL. Use the schema below to answer the user's latest question.\n"
        f"Schema:\n{schema}\n"
        f"User: {user_message}\n"
        f"Bot:"
    )
    response = model(prompt)

    save_chat(session_id, user_message, response)
    return response

def chatbot_response(session_id, user_message, schema):
    # Load the last few turns of chat history
    history = load_chat_history(session_id)
    # Create a context-aware prompt for the LLM
    prompt = (
        f"{history}"
        f"You are an expert in SQL. Use the schema below to answer the user's latest question.\n"
        f"Schema:\n{schema}\n"
        f"User: {user_message}\n"
        f"Bot:"
    )
    # Get the model's response
    response = model(prompt)
    # Save the interaction to your chat history table
    save_chat(session_id, user_message, response)
    return response

!pip install fastapi uvicorn nest-asyncio

from pyngrok import ngrok

ngrok.set_auth_token("2zXbc9tZzMqXZaMbVwhDONxqJC6_5GpcxAiFbZhw9FYt5xxxx")

!pip install fastapi nest-asyncio uvicorn pyngrok sse-starlette

!pkill ngrok

# ===================== SERVER CODE =====================
import nest_asyncio
import uvicorn
from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse
from ctransformers import AutoModelForCausalLM
from pyngrok import ngrok
import mysql.connector
from datetime import datetime

# Enable asyncio in Colab or Jupyter
nest_asyncio.apply()

# Initialize FastAPI app
app = FastAPI()

# Set ngrok token and expose port 8090
ngrok.set_auth_token("2zXbc9tZzMqXZaMbVwhDONxqJC6_5GpcxAiFbZhw9FYtxxxxx")
public_url = ngrok.connect(8090)
print("FastAPI Server Public URL:", public_url)

# Connect to MariaDB
conn = mysql.connector.connect(
    host="0.tcp.in.ngrok.io",
    port=14438,
    user="root",
    password="diya@321",
    database="office"
)
cursor = conn.cursor()

print("Loading model")
model = AutoModelForCausalLM.from_pretrained(
    "/content/mistral-gguf",
    model_file="mistral-7b-instruct-v0.1.Q4_K_M.gguf",
    model_type="mistral"
)
print("Model loaded.")

# SQL Schema string
schema = """CREATE VIEW shipment_summary (
    product_id INT,
    product_code VARCHAR(10),
    product_name VARCHAR(50),
    product_type VARCHAR(50),
    product_site VARCHAR(50),
    YEAR INT,
    qtr VARCHAR(5),
    month VARCHAR(15),
    plan DECIMAL(10,2),
    actual DECIMAL(10,2)
);"""

# Ensure chat_history table exists
cursor.execute("""
CREATE TABLE IF NOT EXISTS chat_history (
    session_id VARCHAR(255),
    user_message TEXT,
    bot_response TEXT,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (session_id, timestamp)
) ENGINE=InnoDB;
""")
conn.commit()

# Save chat in DB
def save_chat(session_id, user_message, bot_response):
    insert_query = """
        INSERT INTO chat_history (session_id, user_message, bot_response)
        VALUES (%s, %s, %s)
    """
    cursor.execute(insert_query, (session_id, user_message, bot_response))
    conn.commit()

# Load last few chat turns
def load_chat_history(session_id, max_turns=5):
    select_query = """
        SELECT user_message, bot_response FROM chat_history
        WHERE session_id=%s
        ORDER BY timestamp DESC
        LIMIT %s
    """
    cursor.execute(select_query, (session_id, max_turns))
    rows = cursor.fetchall()[::-1]
    history = ""
    for user_msg, bot_resp in rows:
        history += f"User: {user_msg}\nBot: {bot_resp}\n"
    return history

# Streaming response generator
def stream_response(session_id, user_message):
    history = load_chat_history(session_id)
    prompt = (
        f"{history}"
        f"You are an expert in SQL. Use the schema below to answer the user's latest question.\n"
        f"Schema:\n{schema}\n"
        f"User: {user_message}\n"
        f"Bot:"
    )
    print(f"\n PROMPT SENT TO MODEL:\n{prompt}\n")

    response_text = ""
    for chunk in model(prompt, stream=True):
        print(chunk, end="", flush=True)
        yield chunk
        response_text += chunk
    save_chat(session_id, user_message, response_text)

# FastAPI /chat route
@app.post("/chat")
async def chat(request: Request):
    data = await request.json()
    session_id = data.get("session_id")
    user_message = data.get("message")

    print(f"\n Received from client | session: {session_id} | message: {user_message}")
    return StreamingResponse(
        stream_response(session_id, user_message),
        media_type="text/plain"
    )

# Start FastAPI server
uvicorn.run(app, port=8090)
